---
layout: post
subclass: post
title: "파이썬 프로그래밍 입문(11) 웹 스크래핑"
date: 2019-05-17 00:00:11
tags: [python-programming, web-scraping]
excerpt: "파이썬 프로그래밍 입문, 웹 스크래핑에 대한 설명입니다."
disqus: True
---

## 웹 스크래핑

웹 스크래핑은 웹에서 데이터를 수집하는 작업 전체를 포괄적으로 말합니다. (API를 활용하는 방법과 사람이 직접 웹 브라우저를 조작하는 방법은 제외합니다.) 방법에 대해서는 교재에서 이미 다루었을 것입니다. 브라우저로 정보를 가져오는 구조와 웹 스크래핑 시의 주의점만 다루고 넘어가도록 하겠습니다.

## 브라우저로 정보를 가져오는 구조

평소 웹을 계속 접하시더라도 실제 브라우저를 사용하면서 그 이면에 일어나는 일은 잘 모를 수 있습니다. 간단하게 살펴보도록 하겠습니다.

1. 사용자(Client)는 웹 브라우저를 사용해 네트워크 요청을 보냅니다.

웹 브라우저 주소창에 네이버 URL 주소를 입력하는 등의 행동을 말합니다. 이렇게 네트워크 요청을 하면 사용자의 컴퓨터에서 요청을 보내게 됩니다. 요청은 1과 0으로 된 데이터(비트 스트림)입니다. 이 데이터에는 헤더와 바디가 포함됩니다.

여기서 헤더는 다음 목적지인 라우터의 MAC주소(장치 주소)와 최종 목적지인 서버(Server)의 IP 주소를 포함합니다. (라우터, MAC주소에 대한 자세한 내용보다는 '요청의 목적지까지 길을 찾기 위한 정보가 헤더라는 이름으로 데이터에 포함되는구나' 정도로 이해하고 넘어가시면 됩니다.)

바디에는 요청하는 내용이 들어갑니다.

2. 라우터가 인터넷으로 전송합니다.

라우터는 데이터가 다음 장치로 이동할 수 있도록 길을 찾아주는 장치입니다. 라우터를 거쳐 인터넷으로 전송됩니다.

3. 여러 경로를 거쳐 서버에 데이터가 도착합니다.

전송받은 데이터의 헤더에서 포트 번호를 확인하고 해당하는 웹 서버 애플리케이션으로 보내줍니다.

4. 웹 서버 애플리케이션에서는 데이터를 전송받습니다.

데이터에는 예를 들어 다음과 같은 정보들이 들어가 있을 수 있습니다. ex) GET 요청/index.html 파일에 대한 요청

5. 웹 서버는 해당하는 HTML 파일을 찾고 새로운 데이터(패킷)로 묶어 사용자의 컴퓨터로 전송합니다. 다시 경로들을 거쳐 사용자의 컴퓨터에 도착합니다.

웹 브라우저는 데이터를 전송하고 받은 데이터를 해석해서 표현하는 어플리케이션이라고 생각할 수 있습니다. 그러므로 브라우저가 아니더라도 파이썬 코드를 통해 데이터를 보내고 받는 작업을 할 수 있습니다. 웹 브라우저를 통하지 않아도 된다는 점과 간략하게나마 주소창에 웹 사이트 주소를 입력했을 때 위와 같은 과정을 거친다는 것을 소개해드리고 싶었습니다.

## 웹 스크래핑 시의 주의점

1. 다양한 예외가 생길 수 있기 때문에 예외 처리를 잘 해주어야 합니다.

예를 들어 페이지를 찾을 수 없는 경우도 있고, 찾고자 하는 태그가 없는 경우도 있을 수 있습니다. 다양한 예외가 생길 수 있기 때문에 이러한 상황들을 대응할 수 있는 예외 처리 코드를 삽입해주어야 합니다.

2. 사이트의 구조가 변경되면 프로그램이 정상적으로 동작하지 않을 수 있습니다.

예상하셨겠지만 사이트의 구조가 변경되면 프로그램이 정상적으로 동작하지 않을 수 있습니다. 그러므로 다음 같은 경우에는 쉽게 프로그램이 정상적으로 동작하지 않을 수 있습니다.

```python
bsObj.findAll("table")[4].findAll("tr")[2].find("td").findAll("div")[1].find("a")
```

엄청 깊숙이 있는 데이터를 찾는 코드입니다. 많은 책들에서 이렇게 데이터가 깊숙이 파묻혀 있고 정형화되지 않은 경우 다른 방법을 찾는 것이 좋다고 언급합니다.

3. 웹 스크래핑이 가능하다는 것과 웹 스크래핑을 해야 하는 것은 다른 이야기입니다.

무작정 웹 스크래핑을 시도할 것이 아니라 해당 데이터를 접근하기 위한 API 등이 제공되고 더욱 적합한 방법이 있다면 그 방법을 사용하시면 됩니다. 더 나은 대안이 있는지 고려해보고 없다면 웹 스크래핑을 택하시는 것을 추천합니다.

4. 웹 스크래핑을 할 때 타겟 서버에 의해 봇으로 분류되어 차단당할 수 있다.

사람이 클릭해서 데이터를 요청하는 것과 다르게 웹 스크래핑 프로그램은 엄청나게 빠른 속도로 수많은 요청을 합니다. 타겟 서버 입장에서도 이러한 봇들이 많아지면 부하가 커지는 것이 당연합니다. 그러므로 타겟 서버에서 대개 이러한 봇들을 차단하는 경우가 많습니다.

이러한 상황을 피하려면 몇 가지 주의해야 할 점이 있습니다.

- 요청의 헤더 값을 변경해준다.

파이썬의 urllib 라이브러리를 사용해서 요청을 보낼 경우 헤더 값은 아래와 같습니다.

```
User-Agent: Python-urllib/3.4
```

위와 같은 방식입니다. 하지만 일반적인 사용자가 요청을 보낼 경우 아래와 같은 형식입니다.

```
User-Agent: Mozila/5.0 (Macintosh; Intel Mac OS X 10_9_5)
AppleWebkit/537.36 (KHTML, like Gecko)
Chrome/39.0.2171.95 Safari/537.36
```

requests 라이브러리를 사용하면 헤더의 필드값을 변경할 수 있습니다. 일반적인 사용자의 요청처럼 헤더를 변경하는 것이 차단을 당하지 않는 하나의 방법입니다.

- 데이터 요청에 간격을 준다.

웹 스크래핑 프로그램의 요청은 컴퓨터가 보내기 때문에 상당히 짧은 시간에 수많은 요청을 보냅니다. 타겟 서버에서는 사람의 일반적인 클릭과 다르게 엄청나게 빠르고 많은 요청을 구별해서 차단하기도 합니다.

```python
time.sleep(3)
```

위의 코드처럼 요청을 보내는 작업 도중에 sleep 함수를 통해 3초 또는 몇 초간 동작을 멈추고 다시 작업을 진행하도록 하는 것도 방법입니다.

그 밖에 대체 IP 주소를 얻기 위해 원격 서버를 활용하는 등의 방법도 있다고 합니다. 위의 주의점을 잘 숙지하시고 웹 스크래핑을 활용하시면 굉장히 강력한 도구가 될 것이라고 생각합니다.
