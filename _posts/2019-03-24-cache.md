---
layout: post
subclass: post
title: "캐시(cahe) 메모리?"
date: 2019-03-24
tags: [computer-science, computer-architecture, cache]
excerpt: "computer-architecture"
disqus: True
---

컴퓨터공학에서 중요한 개념 중 하나인 캐시(Cache)에 대해 살펴보도록 하자. 저장장치와 관련된 개념이지만 캐시을 활용하는 캐싱(caching)은 두루 사용되는 개념이다.

## 캐시(cache)란?

**캐시 메모리(Cache Memory)란 CPU와 주기억장치의 속도 차이를 보완하기 위해 그 사이에 설치하는 반도체 기억장치다.**

## 왜 캐시(cache)를 사용하는가?

CPU가 주기억장치에 접근해 프로그램 코드나 데이터를 인출해오는데 상당히 긴 시간을 CPU가 기다려야 한다. (CPU는 굉장히 비싼 자원인데 말이다.) **이러한 속도 차이로 인한 성능 저하를 방지하기 위해 CPU와 주기억장치 사이에 고속의 기억장치 칩들을 이용한 소용량의 캐시 메모리를 설치해둔다.**

## 캐시 메모리의 동작방식

```
1) CPU가 기억장치로 어떤 데이터를 읽으려고 할 때 먼저 그 데이터가 캐시에 있는지 검사한다.
2) 있다면, 데이터를 즉시 CPU로 인출할 수 있기 때문에 바로 인출한다.(액세스 시간이 크게 단축된다.) 없다면, 주기억장치로부터 인출해온다.(더 오랜 시간이 걸린다. 이 때 액세스했던 부분은 캐시에 복사한다.)
```

여기서 CPU가 원하는 데이터가 이미 캐시에 적재되어 있는 상태를 '캐시 적중(cache hit)'이라고 한다. 반대로 CPU가 원하는 데이터가 캐시에 없는 상태를 '캐시 미스'라고 한다.

기억장치 액세스들 중에서 캐시에 적중되는 비율은 다음과 같다.

```
hit ratio = 캐시에 적중되는 횟수 / 전체기억장치 액세스 횟수
```

평균 기억장치 액세스 시간은 다음과 같다.

```
access time = hit ratio * (cache access time) + (1 - hit ratio) * (cache miss time)
```

## 캐시의 핵심: 지역성

**캐시 적중률이 올라가면 자연히 캐시의 사용 효과가 커진다. 그리고 캐시 적중률은 결국 '지역성(locality)'에 달려있다.**

- 시간적 지역성(temporal locality): 최근에 액세스된 프로그램 코드나 데이터가 가까운 미래에 다시 액세스될 가능성이 높다.
- 공간적 지역성(spatial locality): 기억장치 내에 서로 인접하여 저장되어 있는 데이터들이 연속적으로 액세스될 가능성이 높다.
- 순차적 지역성(sequential locality): 분기가 발생하지 않는 한, 명령어들은 기억장치에 저장된 순서대로 인출되어 실행된다.

프로그램이 수행되는 동안에 실제로 이러한 지역성들 때문에 캐시에 적재되어 있는 정보들이 그 후에 CPU에 의해 다시 사용되는 경우가 많다.

하지만 프로그램의 특성에 따라 지역성이 달라지기 때문에 한계가 있다. 따라서 캐시 설계에 있어 다양한 사항들을 고려해야 한다.

## 캐시 설계에서 고려해야 할 사항들

1. 캐시용량

캐시 용량이 커지면 캐시 적중률은 커진다. 하지만 그에 따라 비용도 증가하고 주변 회로가 복잡해진다.

캐시의 용량과 비용에는 trade-off 관계가 있으므로 이를 잘 고려해서 설계할 필요가 있다.

2. 인출방식

크게 2가지 방식이 있다. 요구 인출(demand fetch)과 선인출(prefetch)이다.

요구 인출(demand fetch)은 필요한 정보만 인출해오는 방법이다.
반면, 선인출(prefetch)은 필요한 정보 외에 필요할 것으로 예측되는 정보도 미리 인출해오는 방법이다.

**선인출 방식은 지역성이 높은 경우 캐시 적중률이 높아진다는 장점이 있지만, 지역성이 낮은 경우에는 불필요한 정보를 읽어오는 데 인출시간만 오히려 길어져 비효율적일 수 있다.**

3. 사상방식

캐시는 주기억장치보다 용량이 훨씬 작다. 그래서 캐시의 라인(슬롯)은 여러 개의 주기억장치 블록들에 의해 공유된다.

이 때 어떤 주기억 장치의 블록들이 어느 캐시 라인을 공유할 것인지를 결정해주는 방법이 필요한데 그 방법을 주기억장치와 캐시간의 '사상 방식(mapping scheme)'라고 한다.

크게 3가지 방법이 있다.

3-1) 직접 사상 방식

주기억장치의 블록들이 지정된 어느 한 캐시 라인으로만 사상될 수 있다. 다시 말하면, 각 블록은 지정된 특정 라인에만 적재될 수 있다.

구현을 위해 주기억 장치의 주소는 세 개의 필드들로 구성된다.

```
태그 필드(t) | 라인 필드(l) | 단어 필드(w)
```

태그 필드: 2^(t+1)개의 블록들 중의 하나를 지정
단어 필드: 2^(w)개의 단어들로 구성된 각 블록 내의 단어들을 구분
라인 필드: m = 2^l / 라인들 중에서 그 블록이 적재될 수 있는 라인을 지정

주기억장치 블록 j가 적재될 수 있는 캐시 라인의 번호 i는 다음의 식에 의해 결정된다.

```
i = j %(mod) m

ex)

캐시 라인의 수 m = 4
주기억장치의 여섯 번째 블록 j = 6

6 % 4 = 2
캐시의 2번 라인에 적재된다.
```

동작방식은 다음과 같다.

```
1) 주기억장치의 주소가 캐시로 보내진다.
2) 주기억장치의 주소 중 라인 필드를 읽고 해당 캐시 라인을 선택한다.
3) 선택된 라인의 태그 비트들과 주소의 태그 비트와 비교된다.
4) 일치하면 캐시 적중, 불일치하면 캐시 미스다.
5) 일치하는 것이 있는 경우, 단어 필드의 값을 이용하여 그 라인에 저장되어 있는 단어들 중 하나를 인출하여 CPU로 전송한다.
```

3-2) 완전-연관 사상

주기억장치의 블록이 캐시의 어떤 라인으로든 적재될 수 있도록 허용해 직접 사상 방식의 단점을 보완한 방식

태그 필드(t) | 단어 필드(w)

태그 값이 주기억장치의 블록 번호와 같다.

```
1) 캐시의 모든 라인들의 태그들과 주기억장치 주소의 태그 필드 내용을 비교하여 일치하는 것을 확인한다.
2) 일치하는 것이 있다면 캐시 적중으로 단어 필드의 값을 이용하여 그 라인에 저장되어 있는 단어들 중의 하나를 인출하여 CPU로 전송한다. (만약 태그가 일치하는 라인이 없다면 캐시 미스다.)
```

기억장치 주소의 태그 비트들은 모든 캐시 라인의 태그들과 비교되어야 한다. 이 때 비교를 하나씩 순차적으로 수행하면 많은 시간이 걸릴 수 있으므로 연관 기억장치 등을 이용하여 비교 동작이 병렬로 신속히 이루어질 수 있도록 구성한다.

라인의 선택이 자유로워 프로그램 세그먼트나 데이터 배열 전체가 캐시로 적재될 수도 있는 장점이 있다.

그러나 모든 태그들을 병렬로 검사하기 위하여 복잡하고 비용이 높은 하드웨어를 포함해야 한다는 결정이 있기 때문에 실제 시스템에서 거의 사용되지 않는다.

3-3) 세트 연관 사상

직접 사상 방식과 완전 연관 사상 방식의 장점만을 취하기 위한 절충안이다.

```
m = v 개의 세트 * k 개의 라인
i(세트 번호) = j(주기억장치 블록 번호) mod v
```

```
태그 필드(t) | 세트 필드(s) | 단어 필드(w)
```

s 개의 세트 비트들은 캐시의 v = 2^s 개의 세트들 중에서 해당 주기억장치 블록이 적재될 수 있는 세트를 선택하는 데 사용된다. 태그 필드의 t 비트들은 그 세트 내에 있는 라인들의 태그들과 비교되어 캐시 적중 여부를 확인하는 데 사용된다.

세트 내 라인의 태그들과 기억장치 주소의 태그를 비교해 일치하는 태그가 있다면 캐시 적중,
그렇지 않으면 캐시 미스가 된다.

세트당 두 개씩의 라인들을 가지는 2-way 조직이 가장 보편적으로 사용되는 세트-연관 조직이다.

4. 교체 알고리즘

캐시 미스가 발생하여 새로운 블록이 주기억장치로부터 캐시로 올라왔을 때 그 블록이 적재될 수 있는 라인들이 이미 다른 블록들로 채워져 있다면, 그 블록들 중의 하나가 교체되어야 한다. (직접 사상에서는 새로운 블록이 적재될 수 있는 라인이 하나뿐이기 때문에 선택의 여지가 없다.)

이 때 블록을 교체하는 방법을 교체 알고리즘이라고 한다. 일반적으로 가장 효과적인 알고리즘이 최소 최근 사용(Least Recently Used: LRU) 알고리즘이다. **더 최근에 사용되었던 데이터들이 앞으로 다시 사용될 가능성이 높다는 가정이 맞는 경우가 실제로 많기 때문에 LRU 알고리즘이 높은 적중률을 보여준다.**

그 외 캐시에 적재된 지 가장 오래된 블록을 교체하는 FIFO(First-In-First-Out) 알고리즘과 최소 사용 빈도(Least Frequently Used: LFU) 알고리즘이 있다.

5. 쓰기 정책

캐시가 적재되어 있는 데이터가 프로그램 처리 과정에서 다른 내용으로 변경되는 경우가 있다. 이때 그 데이터가 캐시에서만 변경되고 주기억장치에서는 갱신되지 않는다면, 캐시와 주기억장치에 있는 그 데이터들이 서로 다른 내용을 가지게 된다.(캐시의 데이터와 주기억장치의 데이터간의 불일치 발생)

**이때 어떤 I/O 장치가 주기억장치로부터 그 데이터를 읽어간다면 잘못된 결과를 초래하게 된다.**

따라서 캐시에 데이터가 변경되면, 그 즉시 주기억장치에 있는 해당 데이터도 갱신하여 같은 값을 가지고 있도록 해야 한다. 그러나 주기억장치에 데이터를 쓰는 동작은 캐시에 비하여 시간이 오래 걸린다. 캐시에서 데이터가 변경될 때마다 주기억장치에도 그 값을 변경하면 기억장치 쓰기 시간이 그만큼 길어진다.

문제를 해결하는 방법은 크게 2가지다.

5-1) 캐시에 데이터가 변경되더라도 주기억장치에는 갱신하지 않는 방법(write-back)

이 방법은 캐시에서만 쓰기 동작이 수행되므로 쓰기 동작에 걸리는 시간이 줄어든다. 대신 교체 알고리즘에 의해 교체되어야 한다면 그 라인의 내용을 주기억장치에 먼저 갱신한 다음 교체되어야 한다.

M(modified) 비트를 두고 캐시의 라인을 교체할 때마다 해당 데이터가 캐시에 적재되어 있는 동안에 수정된(modified) 적이 있는지 반드시 확인해야 한다. 수정된 적이 없으면 바로 교체하면 된다.

I/O 장치가 주기억장치로부터 데이터를 읽어가려는 경우에도 캐시의 M 비트를 먼저 확인한 다음 캐시의 데이터로 내용을 갱신한 다음 읽어가도록 해야 한다.

장점

```
- 기억장치에 대한 쓰기 동작이 최소화된다.
- 쓰기 동작이 캐시까지만 이루어지므로 쓰기 시간이 짧아진다.
```

단점

```
- 캐시 미스가 발생했을 때 캐시의 데이터가 수정된 경우에는 주기억장치의 데이터를 갱신하기 위한 추가적인 시간이 소요된다.
- 캐시에서 수정된 내용이 갱신될 때까지 주기억장치의 해당 블록이 무효 상태에 있게 된다.
- 캐시 라인의 상태를 확인하고 갱신하는 동작들을 지원하기 위한 캐시 제어회로가 복잡해지고,
각 라인이 상태 비트를 가지고 있어야 한다.
```

5-2) 캐시에 데이터가 변경되는대로 주기억장치에도 동시에 갱신하는 방법(write-through)

장점: 주기억장치의 내용이 항상 유효하다.
단점: 쓰기 동작에 걸리는 시간이 길어진다.

이러한 캐시의 원리를 기반으로 웹 캐시 등 다양한 영역에서 캐싱 전략을 활용하고 있다.
흔하게 접하는 웹 캐시에 대하여 조금 더 살펴보고 싶다면 다음 링크를 참고하도록 하자.

[web cache](https://blog.codeanalogies.com/2018/06/11/web-caching-explained-by-buying-milk-at-the-supermarket/)

참고)

컴퓨터구조론 - 김종현님 저자
